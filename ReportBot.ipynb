{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š SPC ReportBot\n",
    "\n",
    "Automated Statistical Process Control (SPC) analysis pipeline that:\n",
    "1. Pulls daily metrics from BigQuery\n",
    "2. Runs Western Electric Rules anomaly detection (XmR charts)\n",
    "3. Generates AI-powered commentary via Gemini\n",
    "4. Publishes to Slack and a web dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import io\n",
    "from google.cloud import bigquery\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# â”€â”€ Project parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "bq_project = 'syb-production-analytics'\n",
    "\n",
    "# â”€â”€ SPC Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class SPCConfig:\n",
    "    \"\"\"Configuration for SPC Analysis and Reporting.\"\"\"\n",
    "\n",
    "    metric_column: str = 'measure'\n",
    "    value_column: str = 'value'\n",
    "    date_column: str = 'partition_date'\n",
    "\n",
    "    metric_labels: Dict[str, str] = field(default_factory=lambda: {\n",
    "        'churned__nr_paying_sound_zones': 'Churned Sound Zones',\n",
    "        'new__nr_paying_sound_zones': 'New Sound Zones',\n",
    "        'resubscribed__nr_paying_sound_zones': 'Resubscribed Sound Zones',\n",
    "        'upsell__nr_paying_sound_zones': 'Upsell Sound Zones',\n",
    "    })\n",
    "\n",
    "    rules_active: List[int] = field(default_factory=lambda: [1, 2, 3, 4, 5])\n",
    "    min_data_points: int = 10\n",
    "\n",
    "    report_title: str = \"Weekly SPC Report\"\n",
    "    team_name: str = \"Soundtrack Analytics\"\n",
    "    notion_doc_url: str = \"\"\n",
    "\n",
    "    colors: Dict[str, str] = field(default_factory=lambda: {\n",
    "        'primary': '#000000',\n",
    "        'mean_line': '#9E6EFF',\n",
    "        'control_limits': '#F23440',\n",
    "        'zone_lines': '#D6C2FF',\n",
    "        'background': '#F5F4F5',\n",
    "        'chart_bg': '#FFFFFF',\n",
    "    })\n",
    "\n",
    "    rule_colors: Dict[str, str] = field(default_factory=lambda: {\n",
    "        'rule_1': '#F23440',\n",
    "        'rule_2': '#FF9800',\n",
    "        'rule_3': '#9E6EFF',\n",
    "        'rule_4': '#1976D2',\n",
    "        'rule_5': '#00897B',\n",
    "    })\n",
    "\n",
    "    def get_metric_label(self, metric_name: str) -> str:\n",
    "        if metric_name in self.metric_labels:\n",
    "            return self.metric_labels[metric_name]\n",
    "        return metric_name.replace('_', ' ').replace('  ', ' ').title()\n",
    "\n",
    "\n",
    "WESTERN_ELECTRIC_RULES = {\n",
    "    'rule_1': {\n",
    "        'name': 'Rule 1: Beyond 3Ïƒ',\n",
    "        'short': 'R1: >3Ïƒ',\n",
    "        'description': 'A single point falls outside the 3Ïƒ control limits',\n",
    "        'business_explanation': 'an extreme value outside normal operating range (beyond 3Ïƒ)',\n",
    "        'severity': 'critical',\n",
    "        'action': 'Investigate immediately â€” likely a special cause event',\n",
    "    },\n",
    "    'rule_2': {\n",
    "        'name': 'Rule 2: 2 of 3 Beyond 2Ïƒ',\n",
    "        'short': 'R2: 2/3 >2Ïƒ',\n",
    "        'description': '2 out of 3 consecutive points fall beyond 2Ïƒ on the same side',\n",
    "        'business_explanation': 'unusual clustering near warning limits (2 of 3 points beyond 2Ïƒ)',\n",
    "        'severity': 'high',\n",
    "        'action': 'Monitor closely â€” early warning of potential shift',\n",
    "    },\n",
    "    'rule_3': {\n",
    "        'name': 'Rule 3: 4 of 5 Beyond 1Ïƒ',\n",
    "        'short': 'R3: 4/5 >1Ïƒ',\n",
    "        'description': '4 out of 5 consecutive points fall beyond 1Ïƒ on the same side',\n",
    "        'business_explanation': 'a subtle but persistent shift pattern (4 of 5 points beyond 1Ïƒ)',\n",
    "        'severity': 'medium',\n",
    "        'action': 'Investigate if pattern continues â€” possible small shift',\n",
    "    },\n",
    "    'rule_4': {\n",
    "        'name': 'Rule 4: 8 Consecutive Same Side',\n",
    "        'short': 'R4: 8 same side',\n",
    "        'description': '8 consecutive points fall on the same side of the center line',\n",
    "        'business_explanation': 'a sustained shift in the process level (8 consecutive points on same side)',\n",
    "        'severity': 'high',\n",
    "        'action': 'Process mean has likely shifted â€” investigate root cause',\n",
    "    },\n",
    "    'rule_5': {\n",
    "        'name': 'Rule 5: 6 Consecutive Trending',\n",
    "        'short': 'R5: 6 trending',\n",
    "        'description': '6 consecutive points trending in the same direction',\n",
    "        'business_explanation': 'a consistent directional trend (6+ consecutive movements)',\n",
    "        'severity': 'medium',\n",
    "        'action': 'Monitor trend â€” may indicate gradual process change',\n",
    "    },\n",
    "    'mr_violation': {\n",
    "        'name': 'MR Violation: Beyond UCL',\n",
    "        'short': 'MR > UCL',\n",
    "        'description': 'Moving Range exceeds Upper Control Limit',\n",
    "        'business_explanation': 'unusual volatility between consecutive data points',\n",
    "        'severity': 'high',\n",
    "        'action': 'Investigate sudden change â€” check for data quality or real event',\n",
    "    },\n",
    "}\n",
    "\n",
    "config = SPCConfig()\n",
    "\n",
    "print(\"âœ… Imports & config loaded\")\n",
    "print(f\"   Active rules: {config.rules_active}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH paying_zone_measures AS (\n",
    "  SELECT\n",
    "    measure,\n",
    "    partition_date,\n",
    "    SUM(value) AS value\n",
    "  FROM `syb-production-analytics.StatisticalProcessControl.SPCKeyMetricsDaily`\n",
    "  WHERE measure LIKE '%nr_paying_sound_zones'\n",
    "    AND measure NOT LIKE '%__eo%'\n",
    "  GROUP BY ALL\n",
    "),\n",
    "\n",
    "max_date AS (\n",
    "  SELECT MAX(partition_date) AS partition_date\n",
    "  FROM paying_zone_measures\n",
    "),\n",
    "\n",
    "dim_measure AS (\n",
    "  SELECT measure\n",
    "  FROM paying_zone_measures\n",
    "  GROUP BY 1\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  SPLIT(dm.measure, 'day__')[SAFE_OFFSET(1)] AS measure,\n",
    "  partition_date,\n",
    "  COALESCE(pzm.value, 0) AS value\n",
    "FROM dim_measure dm\n",
    "CROSS JOIN max_date md\n",
    "CROSS JOIN\n",
    "  UNNEST(\n",
    "    GENERATE_DATE_ARRAY(\n",
    "      DATE_ADD(md.partition_date, INTERVAL - 27 DAY),\n",
    "      md.partition_date\n",
    "    )\n",
    "  ) AS partition_date\n",
    "LEFT JOIN paying_zone_measures pzm USING (\n",
    "  measure,\n",
    "  partition_date\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "client = bigquery.Client(bq_project)\n",
    "df = client.query(query).to_dataframe()\n",
    "print(f\"âœ… Fetched {len(df)} rows, {df['measure'].nunique()} metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SPC Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPCAnalyzer:\n",
    "    \"\"\"Statistical Process Control (SPC) Analysis Engine.\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, config: SPCConfig):\n",
    "        self.df = df.copy()\n",
    "        self.config = config\n",
    "        self._validate_columns()\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _validate_columns(self):\n",
    "        required = [self.config.value_column, self.config.date_column]\n",
    "        if self.config.metric_column:\n",
    "            required.append(self.config.metric_column)\n",
    "        missing = [col for col in required if col not in self.df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        self.df[self.config.date_column] = pd.to_datetime(self.df[self.config.date_column])\n",
    "        self.df[self.config.value_column] = pd.to_numeric(self.df[self.config.value_column], errors='coerce')\n",
    "        self.df = self.df.sort_values(self.config.date_column)\n",
    "\n",
    "    def _calculate_limits(self, series: pd.Series) -> Dict[str, float]:\n",
    "        mr_series = series.diff().abs()\n",
    "        mean_x = series.mean()\n",
    "        mean_mr = mr_series.mean()\n",
    "        sigma_est = mean_mr / 1.128 if mean_mr != 0 else 0\n",
    "        return {\n",
    "            'mean': mean_x, 'std': sigma_est,\n",
    "            'ucl_x': mean_x + (3 * sigma_est), 'lcl_x': mean_x - (3 * sigma_est),\n",
    "            'mean_mr': mean_mr, 'ucl_mr': 3.267 * mean_mr,\n",
    "        }\n",
    "\n",
    "    def _check_rules(self, series: pd.Series, mean: float, std: float) -> pd.DataFrame:\n",
    "        if std == 0:\n",
    "            return pd.DataFrame(False, index=series.index,\n",
    "                                columns=[f'rule_{r}' for r in self.config.rules_active])\n",
    "        z = (series - mean) / std\n",
    "        flags = pd.DataFrame(index=series.index)\n",
    "\n",
    "        if 1 in self.config.rules_active:\n",
    "            flags['rule_1'] = z.abs() > 3\n",
    "        if 2 in self.config.rules_active:\n",
    "            flags['rule_2'] = ((z > 2).astype(int).rolling(3).sum() >= 2) | \\\n",
    "                              ((z < -2).astype(int).rolling(3).sum() >= 2)\n",
    "        if 3 in self.config.rules_active:\n",
    "            flags['rule_3'] = ((z > 1).astype(int).rolling(5).sum() >= 4) | \\\n",
    "                              ((z < -1).astype(int).rolling(5).sum() >= 4)\n",
    "        if 4 in self.config.rules_active:\n",
    "            flags['rule_4'] = ((z > 0).astype(int).rolling(8).sum() == 8) | \\\n",
    "                              ((z < 0).astype(int).rolling(8).sum() == 8)\n",
    "        if 5 in self.config.rules_active:\n",
    "            diffs = series.diff()\n",
    "            flags['rule_5'] = ((diffs > 0).astype(int).rolling(5).sum() == 5) | \\\n",
    "                              ((diffs < 0).astype(int).rolling(5).sum() == 5)\n",
    "        return flags.fillna(False)\n",
    "\n",
    "    def run_analysis(self) -> Dict:\n",
    "        results = {}\n",
    "        if self.config.metric_column and self.config.metric_column in self.df.columns:\n",
    "            groups = self.df.groupby(self.config.metric_column)\n",
    "        else:\n",
    "            groups = [('all_data', self.df)]\n",
    "\n",
    "        for name, group in groups:\n",
    "            clean_group = group.dropna(subset=[self.config.value_column]).copy()\n",
    "            if len(clean_group) < self.config.min_data_points:\n",
    "                continue\n",
    "            stats = self._calculate_limits(clean_group[self.config.value_column])\n",
    "            rule_flags = self._check_rules(clean_group[self.config.value_column], stats['mean'], stats['std'])\n",
    "            clean_group['moving_range'] = clean_group[self.config.value_column].diff().abs()\n",
    "            clean_group['mr_anomaly'] = clean_group['moving_range'] > stats['ucl_mr']\n",
    "            processed_df = pd.concat([clean_group, rule_flags], axis=1)\n",
    "            rule_cols = [c for c in rule_flags.columns if c.startswith('rule_')]\n",
    "            if rule_cols:\n",
    "                processed_df['is_anomaly_x'] = processed_df[rule_cols].any(axis=1)\n",
    "                processed_df['violation_labels_x'] = processed_df[rule_cols].apply(\n",
    "                    lambda row: \",\".join([c.replace('rule_', 'R') for c in rule_cols if row[c]]), axis=1)\n",
    "            else:\n",
    "                processed_df['is_anomaly_x'] = False\n",
    "                processed_df['violation_labels_x'] = \"\"\n",
    "            results[str(name)] = {\n",
    "                'data': processed_df, 'stats': stats,\n",
    "                'metric_name': str(name), 'friendly_name': self.config.get_metric_label(str(name)),\n",
    "            }\n",
    "        return results\n",
    "\n",
    "\n",
    "class SPCChartGenerator:\n",
    "    \"\"\"Generates XmR control charts with brand styling.\"\"\"\n",
    "\n",
    "    def __init__(self, config: SPCConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def _format_value(self, val: float) -> str:\n",
    "        if abs(val) >= 1000:   return f\"{val:,.0f}\"\n",
    "        elif abs(val) >= 10:   return f\"{val:.1f}\"\n",
    "        else:                  return f\"{val:.2f}\"\n",
    "\n",
    "    def plot_metric(self, data: Dict, save_path: Optional[str] = None) -> plt.Figure:\n",
    "        df = data['data']\n",
    "        stats = data['stats']\n",
    "        friendly_name = data['friendly_name']\n",
    "        date_col = self.config.date_column\n",
    "        value_col = self.config.value_column\n",
    "\n",
    "        fig, (ax_x, ax_mr) = plt.subplots(2, 1, figsize=(14, 10), sharex=True,\n",
    "                                           gridspec_kw={'height_ratios': [3, 1]})\n",
    "        fig.patch.set_facecolor(self.config.colors['background'])\n",
    "        for ax in [ax_x, ax_mr]:\n",
    "            ax.set_facecolor(self.config.colors['chart_bg'])\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            ax.tick_params(colors=self.config.colors['primary'], labelsize=9)\n",
    "        ax_x.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "\n",
    "        # X chart\n",
    "        ax_x.plot(df[date_col], df[value_col], color=self.config.colors['primary'],\n",
    "                  linewidth=1.5, marker='o', markersize=4, zorder=3, label='Value')\n",
    "        ax_x.axhline(stats['mean'], color=self.config.colors['mean_line'], linewidth=2, label='XÌ„ (Mean)')\n",
    "        ax_x.axhline(stats['ucl_x'], color=self.config.colors['control_limits'],\n",
    "                     linestyle='--', linewidth=1.5, label='UCL / LCL (Â±3Ïƒ)')\n",
    "        ax_x.axhline(stats['lcl_x'], color=self.config.colors['control_limits'], linestyle='--', linewidth=1.5)\n",
    "        sigma = stats['std']\n",
    "        ax_x.axhline(stats['mean'] + 1.5 * sigma, color=self.config.colors['zone_lines'],\n",
    "                     linestyle=':', alpha=0.8, linewidth=1, label='Â±1.5Ïƒ')\n",
    "        ax_x.axhline(stats['mean'] - 1.5 * sigma, color=self.config.colors['zone_lines'],\n",
    "                     linestyle=':', alpha=0.8, linewidth=1)\n",
    "\n",
    "        # Anomaly markers\n",
    "        rule_cols = sorted([c for c in df.columns if c.startswith('rule_')])\n",
    "        plotted_points = set()\n",
    "        for rule_col in rule_cols:\n",
    "            rule_points = df[df[rule_col] == True]\n",
    "            new_points = rule_points[~rule_points.index.isin(plotted_points)]\n",
    "            if new_points.empty:\n",
    "                continue\n",
    "            color = self.config.rule_colors.get(rule_col, self.config.colors['control_limits'])\n",
    "            label = WESTERN_ELECTRIC_RULES.get(rule_col, {}).get('short', rule_col)\n",
    "            ax_x.scatter(new_points[date_col], new_points[value_col], color=color, s=100,\n",
    "                        zorder=10 + rule_cols.index(rule_col), marker='o',\n",
    "                        edgecolors='white', linewidths=1, label=label)\n",
    "            plotted_points.update(new_points.index)\n",
    "\n",
    "        for _, row in df[df['is_anomaly_x']].iterrows():\n",
    "            triggered = sorted([c for c in rule_cols if row.get(c, False)])\n",
    "            if triggered:\n",
    "                color = self.config.rule_colors.get(triggered[0], self.config.colors['control_limits'])\n",
    "                val_str = self._format_value(row[value_col])\n",
    "                ax_x.annotate(f\"{row['violation_labels_x']}\\n({val_str})\",\n",
    "                             (row[date_col], row[value_col]), xytext=(0, 14),\n",
    "                             textcoords='offset points', ha='center', fontsize=7,\n",
    "                             color=color, fontweight='bold')\n",
    "\n",
    "        ax_x.set_title(f\"XmR Chart: {friendly_name}\", fontsize=14, fontweight='bold',\n",
    "                       color=self.config.colors['primary'], pad=15)\n",
    "        ax_x.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=8, frameon=False)\n",
    "\n",
    "        # MR chart\n",
    "        ax_mr.plot(df[date_col], df['moving_range'], color=self.config.colors['primary'],\n",
    "                   linewidth=1.5, marker='o', markersize=4, label='Moving Range', zorder=3)\n",
    "        ax_mr.axhline(stats['mean_mr'], color=self.config.colors['mean_line'], linewidth=1.5, label='MRÌ„ (Mean)')\n",
    "        ax_mr.axhline(stats['ucl_mr'], color=self.config.colors['control_limits'],\n",
    "                      linestyle='--', linewidth=1.5, label='UCL MR')\n",
    "        anomalies_mr = df[df['mr_anomaly']]\n",
    "        if not anomalies_mr.empty:\n",
    "            ax_mr.scatter(anomalies_mr[date_col], anomalies_mr['moving_range'],\n",
    "                         color=self.config.rule_colors['rule_1'], s=90, zorder=10,\n",
    "                         marker='o', edgecolors='white', linewidths=1, label='R1: MR > UCL')\n",
    "            for _, row in anomalies_mr.iterrows():\n",
    "                val_str = self._format_value(row['moving_range'])\n",
    "                ax_mr.annotate(f\"R1\\n({val_str})\", (row[date_col], row['moving_range']),\n",
    "                              xytext=(0, 12), textcoords='offset points', ha='center',\n",
    "                              fontsize=7, color=self.config.rule_colors['rule_1'], fontweight='bold')\n",
    "        ax_mr.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=8, frameon=False)\n",
    "        ax_mr.xaxis.set_major_formatter(mdates.DateFormatter('%b %-d'))\n",
    "        ax_mr.xaxis.set_major_locator(mdates.DayLocator(interval=3))\n",
    "        fig.autofmt_xdate(rotation=30, ha='right')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            fig.savefig(save_path, dpi=150, bbox_inches='tight', facecolor=self.config.colors['background'])\n",
    "            plt.close(fig)\n",
    "            return None\n",
    "        return fig\n",
    "\n",
    "    def plot_all(self, results: Dict, save_dir: Optional[str] = None) -> Dict[str, plt.Figure]:\n",
    "        figures = {}\n",
    "        if save_dir and not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        for metric_name, data in results.items():\n",
    "            if save_dir:\n",
    "                safe_name = \"\".join([c if c.isalnum() else \"_\" for c in metric_name])\n",
    "                self.plot_metric(data, save_path=os.path.join(save_dir, f\"SPC_{safe_name}.png\"))\n",
    "            else:\n",
    "                figures[metric_name] = self.plot_metric(data)\n",
    "        return figures\n",
    "\n",
    "\n",
    "class SPCCommentaryGenerator:\n",
    "    \"\"\"Generates natural-language commentary from SPC results.\"\"\"\n",
    "\n",
    "    def __init__(self, results: Dict, config: SPCConfig):\n",
    "        self.results = results\n",
    "        self.config = config\n",
    "\n",
    "    def _assess_status(self, data: Dict) -> str:\n",
    "        df = data['data']\n",
    "        has_r1 = df.get('rule_1', pd.Series([False])).any()\n",
    "        mr_anomalies = df['mr_anomaly'].sum()\n",
    "        x_anomalies = df['is_anomaly_x'].sum()\n",
    "        if has_r1 or mr_anomalies > 1:\n",
    "            return 'alert'\n",
    "        elif x_anomalies > 0:\n",
    "            return 'warning'\n",
    "        return 'stable'\n",
    "\n",
    "    def _get_triggered_rules(self, data: Dict) -> List[Dict]:\n",
    "        df = data['data']\n",
    "        triggered = []\n",
    "        rule_cols = [c for c in df.columns if c.startswith('rule_')]\n",
    "        for rule_col in rule_cols:\n",
    "            if df[rule_col].any():\n",
    "                rule_info = WESTERN_ELECTRIC_RULES.get(rule_col, {})\n",
    "                triggered_rows = df[df[rule_col]]\n",
    "                triggered.append({\n",
    "                    'rule': rule_col,\n",
    "                    'name': rule_info.get('name', rule_col),\n",
    "                    'short': rule_info.get('short', rule_col),\n",
    "                    'description': rule_info.get('description', ''),\n",
    "                    'business_explanation': rule_info.get('business_explanation', ''),\n",
    "                    'severity': rule_info.get('severity', 'medium'),\n",
    "                    'action': rule_info.get('action', ''),\n",
    "                    'count': int(df[rule_col].sum()),\n",
    "                    'dates': triggered_rows[self.config.date_column].dt.strftime('%b %d').tolist(),\n",
    "                    'values': triggered_rows[self.config.value_column].tolist(),\n",
    "                })\n",
    "        if df['mr_anomaly'].any():\n",
    "            mr_info = WESTERN_ELECTRIC_RULES.get('mr_violation', {})\n",
    "            mr_rows = df[df['mr_anomaly']]\n",
    "            triggered.append({\n",
    "                'rule': 'mr_violation',\n",
    "                'name': mr_info.get('name', 'MR Violation'),\n",
    "                'short': mr_info.get('short', 'MR > UCL'),\n",
    "                'description': mr_info.get('description', ''),\n",
    "                'business_explanation': mr_info.get('business_explanation', ''),\n",
    "                'severity': mr_info.get('severity', 'high'),\n",
    "                'action': mr_info.get('action', ''),\n",
    "                'count': int(df['mr_anomaly'].sum()),\n",
    "                'dates': mr_rows[self.config.date_column].dt.strftime('%b %d').tolist(),\n",
    "                'values': mr_rows['moving_range'].tolist(),\n",
    "            })\n",
    "        return triggered\n",
    "\n",
    "    def generate_metric_summary(self, metric_name: str) -> Dict:\n",
    "        data = self.results[metric_name]\n",
    "        df = data['data']\n",
    "        stats = data['stats']\n",
    "        friendly_name = data['friendly_name']\n",
    "        status = self._assess_status(data)\n",
    "        triggered_rules = self._get_triggered_rules(data)\n",
    "        current_val = df[self.config.value_column].iloc[-1]\n",
    "        z_score = (current_val - stats['mean']) / stats['std'] if stats['std'] > 0 else 0\n",
    "        position = \"at average\" if abs(z_score) < 0.5 else (\"above average\" if z_score > 0 else \"below average\")\n",
    "        return {\n",
    "            'metric_name': metric_name, 'friendly_name': friendly_name,\n",
    "            'status': status,\n",
    "            'status_emoji': {'alert': 'ğŸš¨', 'warning': 'âš ï¸', 'stable': 'âœ…'}[status],\n",
    "            'current_value': float(current_val), 'position': position,\n",
    "            'mean': float(stats['mean']), 'ucl': float(stats['ucl_x']),\n",
    "            'lcl': float(stats['lcl_x']), 'std': float(stats['std']),\n",
    "            'triggered_rules': triggered_rules,\n",
    "            'is_stable': len(triggered_rules) == 0,\n",
    "            'recent_values': df[self.config.value_column].tail(7).tolist(),\n",
    "        }\n",
    "\n",
    "    def generate_all_summaries(self) -> List[Dict]:\n",
    "        summaries = [self.generate_metric_summary(name) for name in self.results.keys()]\n",
    "        status_order = {'alert': 0, 'warning': 1, 'stable': 2}\n",
    "        return sorted(summaries, key=lambda x: status_order[x['status']])\n",
    "\n",
    "    def format_slack_message(self, summaries: List[Dict], executive_summary: Optional[str] = None) -> str:\n",
    "        from datetime import datetime\n",
    "        today = datetime.now().strftime('%B %d, %Y')\n",
    "        lines = [f\"ğŸ“Š *{self.config.report_title} â€” {today}*\", \"\"]\n",
    "        if executive_summary:\n",
    "            lines.extend([\"*Executive Summary*\", executive_summary, \"\"])\n",
    "        lines.append(\"â”€\" * 40)\n",
    "        for s in summaries:\n",
    "            lines.append(\"\")\n",
    "            lines.append(f\"{s['status_emoji']} *{s['friendly_name']}*: {s['current_value']:,.0f} ({s['position']})\")\n",
    "            lines.append(f\"   Mean: {s['mean']:,.1f} | Limits: [{s['lcl']:,.1f}, {s['ucl']:,.1f}]\")\n",
    "            if s['triggered_rules']:\n",
    "                for rule in s['triggered_rules']:\n",
    "                    lines.append(f\"   â€¢ *{rule['short']}* on {', '.join(rule['dates'][-3:])} â€” {rule['business_explanation']}\")\n",
    "            else:\n",
    "                lines.append(\"   â€¢ No signals detected â€” process is stable\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"â”€\" * 40)\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"*Western Electric Rules Reference:*\")\n",
    "        for rule_key in ['rule_1', 'rule_2', 'rule_3', 'rule_4', 'rule_5']:\n",
    "            if int(rule_key.split('_')[1]) in self.config.rules_active:\n",
    "                rule = WESTERN_ELECTRIC_RULES[rule_key]\n",
    "                lines.append(f\"â€¢ {rule['short']}: {rule['description']}\")\n",
    "        lines.append(\"\")\n",
    "        if self.config.notion_doc_url:\n",
    "            lines.append(f\"ğŸ“– <{self.config.notion_doc_url}|Read the full SPC documentation>\")\n",
    "            lines.append(\"\")\n",
    "        lines.append(f\"_Generated by {self.config.team_name}_\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def prepare_llm_context(self) -> str:\n",
    "        summaries = self.generate_all_summaries()\n",
    "        return json.dumps(summaries, indent=2, default=str)\n",
    "\n",
    "\n",
    "def run_spc_analysis(df: pd.DataFrame, config: Optional[SPCConfig] = None) -> Dict:\n",
    "    \"\"\"Run complete SPC analysis pipeline.\"\"\"\n",
    "    if config is None:\n",
    "        config = SPCConfig()\n",
    "    analyzer = SPCAnalyzer(df, config)\n",
    "    results = analyzer.run_analysis()\n",
    "    chart_gen = SPCChartGenerator(config)\n",
    "    figures = chart_gen.plot_all(results)\n",
    "    for metric_name in results:\n",
    "        if metric_name in figures:\n",
    "            results[metric_name]['figure'] = figures[metric_name]\n",
    "    commentary_gen = SPCCommentaryGenerator(results, config)\n",
    "    return {'results': results, 'figures': figures, 'commentary': commentary_gen, 'config': config}\n",
    "\n",
    "print(\"âœ… SPC engine loaded (SPCAnalyzer, SPCChartGenerator, SPCCommentaryGenerator)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Analysis & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete analysis pipeline\n",
    "output = run_spc_analysis(df, config)\n",
    "results = output['results']\n",
    "figures = output['figures']\n",
    "commentary = output['commentary']\n",
    "\n",
    "print(f\"âœ… Analysis complete for {len(results)} metrics:\")\n",
    "for name, data in results.items():\n",
    "    status = commentary._assess_status(data)\n",
    "    emoji = {'alert': 'ğŸš¨', 'warning': 'âš ï¸', 'stable': 'âœ…'}[status]\n",
    "    print(f\"   {emoji} {data['friendly_name']}\")\n",
    "\n",
    "# Display charts\n",
    "for fig in figures.values():\n",
    "    plt.show()\n",
    "\n",
    "# â”€â”€ Anomalies table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_anomalies = []\n",
    "for metric_name, data in results.items():\n",
    "    df_metric = data['data']\n",
    "    anomalies = df_metric[df_metric['is_anomaly_x'] | df_metric['mr_anomaly']].copy()\n",
    "    anomalies['metric'] = data['friendly_name']\n",
    "    all_anomalies.append(anomalies)\n",
    "\n",
    "if all_anomalies:\n",
    "    anomalies_df = pd.concat(all_anomalies, ignore_index=True)\n",
    "    display_cols = ['metric', config.date_column, config.value_column, 'moving_range',\n",
    "                    'violation_labels_x', 'mr_anomaly']\n",
    "    rule_cols = [f'rule_{i}' for i in config.rules_active if f'rule_{i}' in anomalies_df.columns]\n",
    "    display_cols.extend(rule_cols)\n",
    "    print(f\"\\nğŸ“‹ All Anomalies ({len(anomalies_df)} total):\")\n",
    "    display(anomalies_df[display_cols])\n",
    "else:\n",
    "    print(\"\\nâœ… No anomalies detected across all metrics!\")\n",
    "\n",
    "# â”€â”€ Template-based report (no LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summaries = commentary.generate_all_summaries()\n",
    "slack_message = commentary.format_slack_message(summaries)\n",
    "print(\"\\n\" + slack_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM Commentary (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load secrets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "load_dotenv(override=True)\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = os.environ.get('GOOGLE_CLOUD_PROJECT', bq_project)\n",
    "GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')\n",
    "SLACK_BOT_TOKEN = os.environ.get('SLACK_BOT_TOKEN')\n",
    "SLACK_CHANNEL = os.environ.get('SLACK_CHANNEL', '#spc-reports')\n",
    "DASHBOARD_API_URL = os.environ.get('DASHBOARD_API_URL', 'https://eeeubeqjehgfltuqvezw.supabase.co/functions/v1/ingest-spc-data')\n",
    "DASHBOARD_API_KEY = os.environ.get('DASHBOARD_API_KEY', 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVlZXViZXFqZWhnZmx0dXF2ZXp3Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzA4ODM3ODQsImV4cCI6MjA4NjQ1OTc4NH0.W9nzLjsH0Umd01DAqcxbwd4YBiAB5lsHwnEMAq7c6WA')\n",
    "\n",
    "SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')\n",
    "\n",
    "print(f\"Gemini API Key: {'âœ…' if GEMINI_API_KEY else 'âŒ'}\")\n",
    "print(f\"Slack Bot Token: {'âœ…' if SLACK_BOT_TOKEN else 'âŒ'}\")\n",
    "print(f\"Slack Channel: {SLACK_CHANNEL}\")\n",
    "print(f\"Dashboard API: {'âœ…' if DASHBOARD_API_URL else 'âŒ'}\")\n",
    "\n",
    "# â”€â”€ Build prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def build_slack_bullet_prompt(commentary) -> str:\n",
    "    \"\"\"Build prompt that generates conversational, narrative-style summary.\"\"\"\n",
    "    context = commentary.prepare_llm_context()\n",
    "    rules_ref = \"\\n\".join([\n",
    "        f\"- {WESTERN_ELECTRIC_RULES[f'rule_{i}']['short']}: {WESTERN_ELECTRIC_RULES[f'rule_{i}']['description']}\"\n",
    "        for i in commentary.config.rules_active\n",
    "    ])\n",
    "    return f\"\"\"You are a data analyst writing a weekly metrics update for your team in Slack. Your audience includes both technical and non-technical colleagues (execs, managers, analysts).\n",
    "\n",
    "CONTEXT:\n",
    "- These are Statistical Process Control (SPC) metrics tracking sound zone subscriptions\n",
    "- Signals indicate when a metric is behaving unusually (outside normal variation)\n",
    "- Rules reference:\n",
    "{rules_ref}\n",
    "- MR_violation = unusual volatility between consecutive data points\n",
    "\n",
    "METRICS DATA:\n",
    "{context}\n",
    "\n",
    "TASK:\n",
    "Write a conversational, narrative-style metrics update following this structure:\n",
    "\n",
    "**Opening (1-2 sentences):**\n",
    "- Friendly greeting (e.g., \"Happy [Day] team!\")\n",
    "- Brief overall sentiment about the week\n",
    "\n",
    "**Context paragraph (if relevant):**\n",
    "- Any notable patterns, investigations, or context that explains the data\n",
    "- Keep it brief but insightful\n",
    "\n",
    "**TL;DR**\n",
    "Bullet points with concise summaries of each metric. Start each bullet with an appropriate status emoji (âœ… for stable, âš ï¸ for warning, ğŸš¨ for critical), then the metric name in bold:\n",
    "â€¢ [emoji] *[Metric Name]*: [One sentence describing the pattern and what it means]\n",
    "[repeat for each metric]\n",
    "\n",
    "**Breakdown of metrics**\n",
    "For each metric, write a short narrative paragraph covering:\n",
    "- What you're observing in the data (trends, patterns, anomalies)\n",
    "- Context or explanation for what's happening\n",
    "- Assessment of control limits and stability\n",
    "- Any specific rule violations and what they indicate\n",
    "- Business implications if relevant\n",
    "\n",
    "**Closing:**\n",
    "\"If you have any questions, please let me know!\"\n",
    "\n",
    "STYLE RULES:\n",
    "- Write like you're talking to colleagues, not reading from a textbook\n",
    "- Use contractions where natural (it's, we're, there's)\n",
    "- Be specific with numbers and dates when referencing anomalies\n",
    "- Explain SPC rules in plain language (e.g., \"showing a sustained upward trend\" instead of just \"Rule 5\")\n",
    "- When metrics are stable, say so clearly: \"within control limits\" or \"no concerning signals\"\n",
    "- When there are signals, explain what they mean in business terms\n",
    "- Use *bold* for metric names and key findings\n",
    "- Sound thoughtful and analytical, not robotic\n",
    "- Keep paragraphs digestible (2-4 sentences each)\n",
    "\n",
    "Write ONLY the metrics update in the format above, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "custom_prompt = build_slack_bullet_prompt(commentary)\n",
    "print(f\"âœ… Prompt built ({len(custom_prompt):,} chars)\")\n",
    "\n",
    "# â”€â”€ Call Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def generate_llm_summary_gemini(prompt: str, api_key: str = None, project: str = None) -> str:\n",
    "    \"\"\"Call Gemini API to generate the summary.\"\"\"\n",
    "    if api_key:\n",
    "        from google import genai\n",
    "        client = genai.Client(api_key=api_key)\n",
    "        response = client.models.generate_content(model='gemini-2.0-flash', contents=prompt)\n",
    "        return response.text\n",
    "    elif project:\n",
    "        import vertexai\n",
    "        from vertexai.generative_models import GenerativeModel\n",
    "        vertexai.init(project=project, location=\"us-central1\")\n",
    "        model = GenerativeModel(\"gemini-2.0-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    else:\n",
    "        raise ValueError(\"Provide either api_key or project\")\n",
    "\n",
    "\n",
    "print(\"ğŸ¤– Calling Gemini to generate summary...\")\n",
    "try:\n",
    "    if GEMINI_API_KEY:\n",
    "        executive_summary = generate_llm_summary_gemini(custom_prompt, api_key=GEMINI_API_KEY)\n",
    "    else:\n",
    "        executive_summary = generate_llm_summary_gemini(custom_prompt, project=GOOGLE_CLOUD_PROJECT)\n",
    "    print(\"âœ… Summary generated!\\n\")\n",
    "    print(executive_summary)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"\\nTo fix this, either:\")\n",
    "    print(\"1. Set GEMINI_API_KEY from https://aistudio.google.com/apikey\")\n",
    "    print(\"2. Or ensure Vertex AI API is enabled in your GCP project\")\n",
    "    executive_summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Slack helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def upload_chart(headers: dict, fig, metric_name: str, config) -> str | None:\n",
    "    \"\"\"Upload a chart to Slack. Returns file_id.\"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi=150, bbox_inches='tight', facecolor=config.colors['background'])\n",
    "    buf.seek(0)\n",
    "    content = buf.read()\n",
    "    safe_name = \"\".join([c if c.isalnum() else \"_\" for c in metric_name])\n",
    "    resp = requests.post(\"https://slack.com/api/files.getUploadURLExternal\", headers=headers,\n",
    "                        data={\"filename\": f\"SPC_{safe_name}.png\", \"length\": len(content)})\n",
    "    if not resp.json().get(\"ok\"):\n",
    "        return None\n",
    "    requests.post(resp.json()[\"upload_url\"], files={\"file\": (f\"SPC_{safe_name}.png\", content, \"image/png\")})\n",
    "    buf.close()\n",
    "    return resp.json()[\"file_id\"]\n",
    "\n",
    "\n",
    "def resolve_channel_id(headers: dict, channel: str) -> str | None:\n",
    "    \"\"\"Get channel ID by posting and deleting a test message.\"\"\"\n",
    "    resp = requests.post(\"https://slack.com/api/chat.postMessage\", headers=headers,\n",
    "                        json={\"channel\": channel, \"text\": \"...\"})\n",
    "    if resp.json().get(\"ok\"):\n",
    "        channel_id = resp.json()[\"channel\"]\n",
    "        requests.post(\"https://slack.com/api/chat.delete\", headers=headers,\n",
    "                     json={\"channel\": channel_id, \"ts\": resp.json()[\"ts\"]})\n",
    "        return channel_id\n",
    "    return None\n",
    "\n",
    "\n",
    "def post_to_slack(bot_token: str, channel: str, message: str, figures: dict, config) -> bool:\n",
    "    \"\"\"Post message with charts attached to a single message.\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {bot_token}\"}\n",
    "    file_ids = [{\"id\": fid, \"title\": f\"XmR: {config.get_metric_label(n)}\"}\n",
    "                for n, fig in figures.items() if fig and (fid := upload_chart(headers, fig, n, config))]\n",
    "    if not file_ids:\n",
    "        resp = requests.post(\"https://slack.com/api/chat.postMessage\", headers=headers,\n",
    "                            json={\"channel\": channel, \"text\": message})\n",
    "        print(\"âœ… Posted\" if resp.json().get(\"ok\") else f\"âŒ {resp.json().get('error')}\")\n",
    "        return resp.json().get(\"ok\", False)\n",
    "    channel_id = channel if channel.startswith(\"C\") else resolve_channel_id(headers, channel)\n",
    "    if not channel_id:\n",
    "        print(f\"âŒ Cannot resolve channel: {channel}\")\n",
    "        return False\n",
    "    resp = requests.post(\"https://slack.com/api/files.completeUploadExternal\", headers=headers,\n",
    "                        json={\"files\": file_ids, \"channel_id\": channel_id, \"initial_comment\": message})\n",
    "    print(f\"âœ… Posted with {len(file_ids)} charts\" if resp.json().get(\"ok\") else f\"âŒ {resp.json().get('error')}\")\n",
    "    return resp.json().get(\"ok\", False)\n",
    "\n",
    "\n",
    "# â”€â”€ Compose & send â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "full_message = commentary.format_slack_message(summaries, executive_summary=executive_summary if executive_summary else None)\n",
    "print(full_message)\n",
    "\n",
    "# âš ï¸ Confirmation required before posting\n",
    "confirm = input(\"\\nâš ï¸  Post to Slack? (yes/y to confirm): \").strip().lower()\n",
    "if confirm in ['yes', 'y']:\n",
    "    if SLACK_BOT_TOKEN:\n",
    "        post_to_slack(SLACK_BOT_TOKEN, SLACK_CHANNEL, full_message, figures, config)\n",
    "    else:\n",
    "        print(\"âŒ Set SLACK_BOT_TOKEN in .env\")\n",
    "else:\n",
    "    print(\"âŒ Cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Dashboard helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def build_dashboard_payload(results: dict, commentary, executive_summary: str, config) -> dict:\n",
    "    \"\"\"Transform SPC results into dashboard API payload format.\"\"\"\n",
    "    from datetime import datetime\n",
    "    first_metric_data = next(iter(results.values()))['data']\n",
    "    latest_date = first_metric_data[config.date_column].max().strftime('%Y-%m-%d')\n",
    "    metrics_payload = []\n",
    "\n",
    "    for metric_name, data in results.items():\n",
    "        df = data['data']\n",
    "        stats = data['stats']\n",
    "        summary = commentary.generate_metric_summary(metric_name)\n",
    "        mean, std = stats['mean'], stats['std']\n",
    "\n",
    "        data_points = [\n",
    "            {\"date\": row[config.date_column].strftime('%Y-%m-%d'),\n",
    "             \"value\": float(row[config.value_column]),\n",
    "             \"moving_range\": float(row['moving_range']) if pd.notna(row['moving_range']) else 0.0,\n",
    "             \"is_anomaly\": bool(row['is_anomaly_x'] or row['mr_anomaly'])}\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        violations = []\n",
    "        for rule in summary['triggered_rules']:\n",
    "            rule_num = rule['rule'].replace('rule_', '') if rule['rule'].startswith('rule_') else 'MR'\n",
    "            severity_map = {'critical': 'critical', 'high': 'critical', 'medium': 'warning', 'low': 'warning'}\n",
    "            for date_str in rule['dates']:\n",
    "                date_obj = pd.to_datetime(f\"{date_str} {latest_date[:4]}\", format='%b %d %Y')\n",
    "                violations.append({\n",
    "                    \"date\": date_obj.strftime('%Y-%m-%d'),\n",
    "                    \"rule_number\": int(rule_num) if rule_num.isdigit() else 0,\n",
    "                    \"rule_name\": rule['short'],\n",
    "                    \"rule_description\": rule['description'],\n",
    "                    \"severity\": severity_map.get(rule['severity'], 'warning'),\n",
    "                })\n",
    "\n",
    "        status_map = {'alert': 'signal', 'warning': 'deteriorating', 'stable': 'stable'}\n",
    "        metrics_payload.append({\n",
    "            \"metric_name\": metric_name, \"metric_label\": data['friendly_name'],\n",
    "            \"mean\": float(mean), \"upper_control_limit\": float(stats['ucl_x']),\n",
    "            \"lower_control_limit\": float(stats['lcl_x']),\n",
    "            \"upper_2sigma\": float(mean + 2 * std), \"lower_2sigma\": float(mean - 2 * std),\n",
    "            \"upper_1sigma\": float(mean + 1 * std), \"lower_1sigma\": float(mean - 1 * std),\n",
    "            \"moving_range_ucl\": float(stats['ucl_mr']),\n",
    "            \"status\": status_map.get(summary['status'], 'stable'),\n",
    "            \"commentary\": executive_summary if executive_summary else \"No commentary generated.\",\n",
    "            \"data_points\": data_points, \"violations\": violations,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"report_title\": f\"{config.report_title} - Week {datetime.now().isocalendar()[1]}\",\n",
    "        \"report_date\": latest_date,\n",
    "        \"team_name\": config.team_name,\n",
    "        \"slack_message\": executive_summary if executive_summary else \"\",\n",
    "        \"metrics\": metrics_payload,\n",
    "    }\n",
    "\n",
    "\n",
    "def post_to_dashboard(payload: dict, api_url: str, api_key: str) -> bool:\n",
    "    \"\"\"POST SPC results to the web dashboard.\"\"\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "    try:\n",
    "        response = requests.post(api_url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(f\"âœ… Dashboard updated: {result.get('report_id', 'success')}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ Dashboard error: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"   Response: {e.response.text}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# â”€â”€ Post to dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dashboard_payload = build_dashboard_payload(results, commentary, executive_summary, config)\n",
    "print(f\"ğŸ“Š Payload ready: {len(dashboard_payload['metrics'])} metrics, {dashboard_payload['report_date']}\")\n",
    "print(f\"   Report: {dashboard_payload['report_title']}\")\n",
    "\n",
    "if DASHBOARD_API_URL and DASHBOARD_API_KEY:\n",
    "    post_to_dashboard(dashboard_payload, DASHBOARD_API_URL, DASHBOARD_API_KEY)\n",
    "else:\n",
    "    print(\"âš ï¸  Dashboard credentials not configured in .env\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxrWr5OOWDa0Q/y8x2LxyP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
